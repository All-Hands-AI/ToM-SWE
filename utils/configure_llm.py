#!/usr/bin/env python3
"""
Configuration script for setting up LLM proxy access.
Helps users configure their environment for the ToM module.
"""

import os
from pathlib import Path

AVAILABLE_MODELS = [
    "claude-3-5-haiku-20241022",
    "claude-3-5-sonnet-20241022",
    "claude-sonnet-4-20250514",
    "claude-opus-4-20250514",
]


def display_available_models():
    """Display available LLM models."""
    print("Available models (all require 'litellm_proxy/' prefix):")
    print("- claude-sonnet-4-20250514 (Most capable)")
    print("- claude-opus-4-20250514 (Most capable)")
    print("- claude-3-7-sonnet-20250219")
    print("- claude-3-5-sonnet-20241022 (Recommended)")
    print("- claude-3-5-sonnet-20240620")
    print("- claude-3-5-haiku-20241022 (Fastest)")
    print()


def get_api_configuration():
    """Get API key and base URL from user."""
    api_key = input("Enter your LITELLM_API_KEY (from Xingyao): ").strip()
    if not api_key:
        print("Error: API key is required")
        return None, None

    base_url = input("Enter LITELLM_BASE_URL (proxy endpoint): ").strip()
    if not base_url:
        print("Using default base URL")
        base_url = "https://your-proxy-endpoint.com"  # Update with actual URL

    return api_key, base_url


def select_default_model():
    """Let user select the default model."""
    print("\nChoose default model:")
    for i, model in enumerate(AVAILABLE_MODELS, 1):
        print(f"{i}. litellm_proxy/{model}")

    try:
        choice = int(input("\nEnter choice (1-4) [default: 2]: ").strip() or "2")
        if 1 <= choice <= len(AVAILABLE_MODELS):
            return f"litellm_proxy/{AVAILABLE_MODELS[choice-1]}"
    except ValueError:
        pass
    return "litellm_proxy/claude-3-5-sonnet-20241022"


def generate_env_content(api_key, base_url, default_model):
    """Generate content for .env file."""
    return f"""# LLM API Configuration for ToM Module
# Generated by configure_llm.py

# API Key for the All Hands AI LLM Proxy
LITELLM_API_KEY={api_key}

# Base URL for the LLM proxy
LITELLM_BASE_URL={base_url}

# Default model to use
DEFAULT_LLM_MODEL={default_model}

# Note: All model names must include the 'litellm_proxy/' prefix
"""


def save_env_file(env_content):
    """Save .env file with user confirmation if it exists."""
    env_path = Path(".env")
    if env_path.exists():
        overwrite = (
            input("\n.env file already exists. Overwrite? (y/N): ").strip().lower()
        )
        if overwrite != "y":
            print("Configuration cancelled.")
            return False

    try:
        with open(env_path, "w") as f:
            f.write(env_content)
        return True
    except Exception as e:
        print(f"Error creating .env file: {e}")
        return False


def display_success_message(api_key, base_url, default_model):
    """Display success message with configuration details."""
    print(f"\n✅ Configuration saved to {Path('.env')}")
    print()
    print("Next steps:")
    print("1. Verify your API key and base URL are correct")
    print("2. Test the configuration: uv run tom-test")
    print("3. Run full analysis: uv run tom-analyze")
    print()
    print("Environment variables set:")
    print(f"  LITELLM_API_KEY: {api_key[:8]}...")
    print(f"  LITELLM_BASE_URL: {base_url}")
    print(f"  DEFAULT_LLM_MODEL: {default_model}")


def create_env_file():
    """Create a .env file with LLM proxy configuration."""
    print("LLM Proxy Configuration Setup")
    print("=" * 40)
    print()

    display_available_models()

    # Get configuration from user
    api_key, base_url = get_api_configuration()
    if not api_key:
        return False

    default_model = select_default_model()
    env_content = generate_env_content(api_key, base_url, default_model)

    if save_env_file(env_content):
        display_success_message(api_key, base_url, default_model)
        return True
    return False


def check_env_variables():
    """Check if required environment variables are set."""
    api_key = os.getenv("LITELLM_API_KEY")
    base_url = os.getenv("LITELLM_BASE_URL")
    model = os.getenv("DEFAULT_LLM_MODEL")

    if api_key:
        print(f"✅ LITELLM_API_KEY: {api_key[:8]}...")
    else:
        print("❌ LITELLM_API_KEY: Not set")

    if base_url:
        print(f"✅ LITELLM_BASE_URL: {base_url}")
    else:
        print("❌ LITELLM_BASE_URL: Not set")

    if model:
        print(f"✅ DEFAULT_LLM_MODEL: {model}")
    else:
        print("❌ DEFAULT_LLM_MODEL: Not set")

    return bool(api_key and base_url)


def check_configuration():
    """Check current configuration status."""
    print("Current LLM Configuration")
    print("=" * 30)

    env_path = Path(".env")
    if env_path.exists():
        print("✅ .env file found")
    else:
        print("❌ .env file not found")

    return check_env_variables()


def display_menu():
    """Display the main menu options."""
    print("Options:")
    print("1. Check current configuration")
    print("2. Set up new configuration")
    print("3. Exit")


def main():
    """Main configuration interface."""
    print("Theory of Mind Module - LLM Configuration")
    print("=" * 45)
    print()
    print("This script helps you configure LLM access for user mental state analysis.")
    print("You'll need an API key from Xingyao for the All Hands AI LLM proxy.")
    print()

    while True:
        display_menu()
        choice = input("\nEnter choice (1-3): ").strip()

        if choice == "1":
            print()
            configured = check_configuration()
            if configured:
                print("\n✅ Configuration looks good!")
            else:
                print("\n❌ Configuration incomplete. Choose option 2 to set up.")
            print()

        elif choice == "2":
            print()
            if create_env_file():
                print("Configuration complete! You can now use the ToM module.")
            break

        elif choice == "3":
            print("Goodbye!")
            break

        else:
            print("Invalid choice. Please enter 1, 2, or 3.")


if __name__ == "__main__":
    main()
